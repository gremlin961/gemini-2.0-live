{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bf6c5b-b6c3-4eea-a07c-9302314065d8",
   "metadata": {},
   "source": [
    "Copyright 2024 Google, LLC. This software is provided as-is,\n",
    "without warranty or representation for any use or purpose. Your\n",
    "use of it is subject to your agreement with Google.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "# Gemini 2.0 Chat Example\n",
    "\n",
    "This notebook provides a simple example for interacting with Google's new Gemini 2.0 models using the unified Gen AI SDK. For more information please visit https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6216210-e56e-4e07-9ef8-c7c077a72e55",
   "metadata": {},
   "source": [
    "Install the needed dependencies for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f7477-c55f-4828-a3c0-3c08213272a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49d241-94f3-4685-94dc-4c0ec21208da",
   "metadata": {},
   "source": [
    "Import python libraries. Note the new \"genai\" library, which you can install using 'pip install google-genai'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55800605-c9b1-40c9-8c12-fb17de2876a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import base64\n",
    "import contextlib\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import wave\n",
    "import itertools\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470424ae-972c-4f13-b3e0-15e50432f5c6",
   "metadata": {},
   "source": [
    "Set your project variables. Change \"YOUR_PROJECT_ID\" to your GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a33169-d938-4185-8a08-d48cbee56958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = \"YOUR_PROJECT_ID\"\n",
    "location = \"global\"\n",
    "region = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b485a0d-aaa3-4f04-b411-3198c3e3d6f8",
   "metadata": {},
   "source": [
    "Initialize the model, specifying 'vertexai=True'. The new Google Gen AI SDK provides a unified interface to Gemini 2.0 through both the Gemini Developer API and the Gemini API on Vertex AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18c7a8-c4e8-4002-bf95-36def8fb580f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(\n",
    "    vertexai=True, project=project_id, location=region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e378b7-93e5-4fd1-a307-3816f0c79269",
   "metadata": {},
   "source": [
    "Instead of performning an interactive chat session, you can optionally request a single turn generated response as outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2347b-403b-4205-b8a6-7bd8eedd69dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = client.models.generate_content(\n",
    "#    model='gemini-2.0-flash-exp', contents='Hi Gemini!'\n",
    "#)\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51680851-4f7e-45f1-9b3f-18b5c86dec1a",
   "metadata": {},
   "source": [
    "## Start with a simple chat session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156e401-d8b4-47b9-8be0-679d0205f0ae",
   "metadata": {},
   "source": [
    "First, we'll select the model we want to use and then set the configuration the response for text only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1bbaf5-deb6-4c18-a300-bd81574bc506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"gemini-2.0-flash-exp\"\n",
    "config = {\"response_modalities\": [\"TEXT\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad97029-deb3-4eea-8156-da255115725e",
   "metadata": {},
   "source": [
    "We need to use nest_asyncio since we will be calling asyncio.run from within the already-running notebook's event loop.  The asyncio.run is used for running a coroutine directly on the event loop, but the event loop will already be running since we're calling it inside of a notebook. You probably will not need this outside of a Jupyter notebook environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ee7aa-bdf6-4548-b928-e905515680e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556959ee-e70c-47e8-a582-f3e407516c24",
   "metadata": {},
   "source": [
    "Define an interactive chat session using the Gen AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b6fab-bb4c-4516-bf76-e2697adda133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def interactive_chat(client, model_id, config):\n",
    "    \"\"\"\n",
    "    Starts an interactive chat session with a specified model.\n",
    "\n",
    "    Args:\n",
    "        client: The client to use for communication.\n",
    "        model_id: The ID of the model to interact with.\n",
    "        config: The configuration to use for the session.\n",
    "    \"\"\"\n",
    "    async with client.aio.live.connect(model=model_id, config=config) as session:\n",
    "        while True:\n",
    "            prompt = input(\"Enter your prompt here (or 'End' to quit): \")\n",
    "            if prompt.lower() == \"end\":\n",
    "                 break\n",
    "\n",
    "            await session.send(prompt, end_of_turn=True)\n",
    "            print(\"> You: \", prompt)  # Print the user's prompt\n",
    "\n",
    "            async for response in session.receive():\n",
    "                if response.text:\n",
    "                    print(response.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d9a92-4ca8-47d8-a0c1-2ec40caf36bc",
   "metadata": {},
   "source": [
    "Start the chat session by running the interactive_chat function. Simply type 'End' to stop the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13fdc4-df58-4f6d-9156-cc008e63e9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asyncio.run(interactive_chat(client, model_id, config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bd440-8d2c-4219-84ea-e18da81896a9",
   "metadata": {},
   "source": [
    " __________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd2779-4dcc-4658-ae05-066cf503c619",
   "metadata": {},
   "source": [
    "That's how easy it is to use the new GenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00ebca-56cc-479e-b35e-a96ced08aff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
